\section*{Limitations}

\paragraph{Reduced PSNR.} Our work reduces the PSNR and MS-SSIM on the test set slightly. This is due to the requirement for a continuous function, which is more difficult to learn than directly predicting from an MLP, which from our qualitative results may not produce coherent movement or isolate non-rigid regions. This can be considered a trade-off between bias and variance, where we reduce the space of learned functions, in order to enforce that they are physically plausible. We argue that this trade-off may be beneficial in some cases, for example if we are interested in controlling movement, it's easier to modify our method's control points than it is to modify the output of the MLP. Our method is also easily interpretable and provably correct and for certain kinds of applications, these qualities would be preferable to quality. For applications which purely focus on reconstruction quality, and do not care about coherent movement or interpretability, it may make more sense to use an MLP.

\paragraph{Bezier Hyperparameters.} While our work is able to capture 3D movement, there are still limitations in what movement it can capture. Specifically, the number spline points determines the degrees of freedom of movement, and correctly selecting the number can be challenging. If too many points are selected, it can suffer from oscillations, akin to Runge's phenomenon, and this was observed during experimentation. If too few points are selected, the space of learnable movement is significantly smaller. For the datasets used and those examined in prior work, simple movement is learned, which protects against these issues by selecting between 4-6 control points, but for more complex movement it is be more difficult.

\paragraph{Transient Content.} This work also does not tackle transient content in a scene. For example, effects such as fire or complex lighting changes beyond moving shadow cannot be accurately captured. This is not a focus of this work, but is orthogonal and important when broadly considering dynamic scenes. While this work does not handle this, in contrast to prior work which may enable learning this by teleporting particles with the deformation network, our approach \textit{prevents} modelling those effects, which is beneficial since an orthogonal component may be able to learn them more effectively.

\paragraph{Reflectance}\label{sec:refl_disc} Another change in our approach is that the reflectance model defined in Eq.~\ref{eq:refl_defn} may not be clearly motivated. It is able to model more than prior work, by limited linear scaling. This may be seen as a pro and con, in that we may be overfitting to the specific dataset we are testing on, but we argue it is broadly viable. In fact, the necessity for this reflectance is that there are non-negligible view-dependent effects in the dataset which a purely positional model cannot capture, but a fully view-dependent model cannot generalize to. Ours fits in the middle, capturing just what is necessary, while preventing content from becoming completely dark from other views. The idea behind this approach was learning a general gamma correction: $A (\text{RGB})^\gamma$, but found that learning $\gamma$ even in the limited range $[0.5,1.5]$ failed due to instability. Thus, we only retained the linear term, and found this to improve performance. We can think of this as a diffuse BSDF, which has an explicit term for diffusion $V\cdot L$, where in our case L is unknown but fixed. In addition, our limitation of $A\in[0.5,1]$ can be considered an implicit bias on the existence of global illumination, in that no particle can be fully black. Hopefully, this sufficiently motivates the modification to prior work's RGB prediction.

\section*{Future Work}

One large next step in neural rendering is to encode dynamic models on top of highly-efficient NeRF representations, such as using a Plenoxel~\cite{yu2021plenoxels}, Instant Neural Graphics Primitives~\cite{mueller2022instant} or other structure to allow for rapid reconstruction of dynamic scenes. Our voxel implementation does not utilize sparsity as much as possible, nor does it use modifications such as hash-encoding, partially to demonstrate the efficacy of our approach on its own two feet. To the author's knowledge, there is no real-time construction of 3D scenes since there did not exist a classical approach to reliably reconstruct movement. Bezier splines may help fill in this gap, requiring a small number of parameters and allowing for efficient rendering and training without requiring a costly MLP evaluation. If this follows the trend of scene reconstruction, it may be multiple orders of magnitude faster to reconstruct dynamic scenes, without loss in quality, and we hope that this work gets adopted for this purpose.

In addition, long duration (minutes or hours) dynamic scene reconstruction has not yet shown to be plausible, but Bezier splines are easily extendable to long scenes by turning them into poly-Bezier splines or adding more control points. It is not immediately clear how to enable efficient reconstruction over long sequences, but using splines is a clear avenue for future work for learning long dynamic scenes with continuity guarantees.

We also hope that future work explores variations on spline formulations, as we select
Bezier splines due to simplicity, numerical stability and efficiency of evaluation. It may be that other splines might have stronger expressivity for reconstruction or other desirable properties, and thus may be useful in different contexts.

\section*{Conclusion}

In conclusion, we devise a new architecture for $C^0, C^1$ continuous interpolation, and show that it works with dynamic NeRF, performing on par with prior work. Our architecture is able to accurately reconstruct scenes, while providing strong smoothness guarantees using a well-studied tool. This leads to coherent movement, which can be observed in a reconstructed video. Hopefully, this inspires more use of classical tools inside of the differentiable rendering pipeline, so we can accurately and efficiently recover physical phenomenon.

While our work is incremental, requiring very little modification to existing code, since it changes the underlying structure to an analytic form, so we expect a large amount of tooling and analytical tools can be built on top of this change, leading to better understanding and analysis of dynamic 3D content.

\iffalse
\section*{Acknowledgements}

Thanks to Elliot Cuzzillo and Seung-Hwan Baek for helping proofread this work.
\fi