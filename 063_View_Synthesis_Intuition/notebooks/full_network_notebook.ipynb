{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hmm4vcJti8k"
   },
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "PEG28Bp3aIAs",
    "outputId": "e94687bf-4dc0-403a-bded-e3036357ff8d"
   },
   "outputs": [],
   "source": [
    "# Check device running the notebook automatically\n",
    "import sys\n",
    "is_on_colab = 'google.colab' in sys.modules\n",
    "is_on_zerus = 'teampc' in sys.argv[0]\n",
    "print(\"Is on colab: \", is_on_colab)\n",
    "print(\"Is on zerus:\", is_on_zerus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVHnBLJjvr-N"
   },
   "source": [
    "## Setup for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1BOhxh7BEYm"
   },
   "outputs": [],
   "source": [
    "if is_on_colab:\n",
    "    # Google Colab setup\n",
    "    \n",
    "    # Mount drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "    # Retrieve repository and cd into root folder\n",
    "    from getpass import getpass\n",
    "    import urllib\n",
    "    import os\n",
    "    os.chdir(\"/content\")\n",
    "    user = input('Github user name: ')\n",
    "    password = getpass('Github password: ')\n",
    "    password = urllib.parse.quote(password) # your password is converted into url format\n",
    "    branch = \"\" # \"-b \" + \"branch_name\"\n",
    "    cmd_string = 'git clone {0} https://{1}:{2}@github.com/lukasHoel/novel-view-synthesis.git'.format(branch, user, password)\n",
    "    os.system(cmd_string)\n",
    "    os.chdir(\"novel-view-synthesis\")\n",
    "\n",
    "    # Install PyTorch3D libraries (required for pointcloud computations.)\n",
    "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VF0QazUmv5lY"
   },
   "source": [
    "## Setup for Local Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTsC8dapAyAD"
   },
   "outputs": [],
   "source": [
    "# ONLY NECESSARY FOR LOCAL EXECUTION (WORKS WITHOUT THIS CELL IN GOOGLE COLAB)\n",
    "# Setup that is necessary for jupyter notebook to find sibling-directories\n",
    "# see: https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "\n",
    "\n",
    "if not is_on_colab:\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd7dyzCGwCZo"
   },
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGfclJp_AyA7"
   },
   "outputs": [],
   "source": [
    "# Imports for this notebook\n",
    "from models.nvs_model import NovelViewSynthesisModel\n",
    "from models.synthesis.synt_loss_metric import SynthesisLoss, SceneEditingLoss, SceneEditingAndSynthesisLoss, SynthesisLossRGBandSeg\n",
    "from util.nvs_solver import NVS_Solver\n",
    "from util.gan_wrapper_solver import GAN_Wrapper_Solver\n",
    "from data.nuim_dataloader import ICLNUIMDataset\n",
    "from data.nuim_dynamics_dataloader import ICLNUIM_Dynamic_Dataset\n",
    "from data.mp3d_dataloader import MP3D_Habitat_Offline_Dataset\n",
    "from data.mp3d_dynamics_dataloader import MP3D_Habitat_Offline_Dynamic_Dataset\n",
    "from projection.z_buffer_manipulator import PtsManipulator\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7R9GHGlhsMQ"
   },
   "outputs": [],
   "source": [
    "# Check training on GPU?\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Training is on GPU with CUDA: {}\".format(cuda))\n",
    "\n",
    "device = \"cuda:0\" if cuda else \"cpu\"\n",
    "\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7mIvRRwJGlR"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Given a model return total number of parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkO15Cr3t3pA"
   },
   "source": [
    "# Load Data\n",
    "Load ICL-NUIM dataset or Matterport3D dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "lQOjeo-oTNny",
    "outputId": "9ec6d88d-e60c-4835-9f34-de6a55c38fd1"
   },
   "outputs": [],
   "source": [
    "dataset_mode = PtsManipulator.matterport_mode\n",
    "#dataset_mode = PtsManipulator.icl_nuim_mode \n",
    "\n",
    "use_dynamics = True\n",
    "\n",
    "image_size = 256\n",
    "\n",
    "print(\"Using dataset: \" + dataset_mode)\n",
    "print(\"Image Size: \" + str(image_size))\n",
    "print(\"Using dynamics: \" + str(use_dynamics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "TcKlv4vsAyBE",
    "outputId": "75776006-facf-46c5-9731-21810ef0f6f6"
   },
   "outputs": [],
   "source": [
    "# Load dataset from drive or local\n",
    "\n",
    "if is_on_colab:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode and not use_dynamics:\n",
    "        path = \"/content/drive/My Drive/Novel_View_Synthesis/matterport3d\"\n",
    "        train_path = path\n",
    "        val_path = path\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and not use_dynamics:\n",
    "        path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/living_room_traj2_loop\"\n",
    "        train_path = path\n",
    "        val_path = path\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and use_dynamics:\n",
    "        train_path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/custom/\"\n",
    "        val_path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/custom/\"\n",
    "\n",
    "        \n",
    "elif is_on_zerus:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode and not use_dynamics:\n",
    "        train_path = \"/mnt/raid/teampc/mp3d_big/train/\"\n",
    "        val_path = \"/mnt/raid/teampc/mp3d_big/val/\"\n",
    "    elif dataset_mode == PtsManipulator.matterport_mode and use_dynamics:\n",
    "        train_path = \"/mnt/raid/teampc/mp3d_dynamics/train\"\n",
    "        val_path = \"/mnt/raid/teampc/mp3d_dynamics/val\"\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and not use_dynamics:\n",
    "        path = \"/mnt/raid/teampc/ICL-NUIM/living_room_traj2_loop\"\n",
    "        train_path = path\n",
    "        val_path = path\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and use_dynamics:\n",
    "        train_path = \"/mnt/raid/teampc/ICL-NUIM/custom_split/train/\"\n",
    "        val_path = \"/mnt/raid/teampc/ICL-NUIM/custom_split/val/\"\n",
    "        \n",
    "else:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode:\n",
    "        path = \"/home/lukas/datasets/Matterport3D/data/v1/tasks/mp3d_habitat/rendered_with_seg\"\n",
    "        train_path = path\n",
    "        val_path = path\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and not use_dynamics:\n",
    "        path = \"/home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop\"\n",
    "        train_path = path\n",
    "        val_path = path\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and use_dynamics:\n",
    "        train_path = \"/home/lukas/Desktop/datasets/ICL-NUIM/custom\"\n",
    "        val_path = \"/home/lukas/Desktop/datasets/ICL-NUIM/custom\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.ToPILImage(), # no longer needed: new dataloader now returns PIL Images\n",
    "    torchvision.transforms.Resize((image_size, image_size)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data_dict = {\n",
    "    \"mode\": dataset_mode,\n",
    "    \"image_size\": image_size,\n",
    "    \"use_dynamics\": use_dynamics,\n",
    "    \"train_path\": train_path,\n",
    "    \"val_path\": val_path,\n",
    "    \"sampleOutput\": True,\n",
    "    \"inverse_depth\": False,\n",
    "    \"cacheItems\": False, # Caching will work only if num_workers = 0. Decide what you like more!\n",
    "}\n",
    "    \n",
    "if dataset_mode == PtsManipulator.matterport_mode and not use_dynamics:\n",
    "    \n",
    "    # THIS IS THE HARDCODED IMAGE SIZE THAT WE SET IN THE HABITAT FRAMEWORK WHEN RENDERING MP3D IMAGES\n",
    "    # THIS DOES NOT CHANGE WHEN WE USE DIFFERENT IMAGE SIZES IN A TRANSFORM OBJECT\n",
    "    # WHEN CHANGING THE IMAGE SIZE IN TRANSFORM OBJECT, THIS GETS REFLECTED IN THE image_size ATTRIBUTE\n",
    "    data_dict['mp3d_image_input_size'] = 256\n",
    "    \n",
    "    if not is_on_zerus:\n",
    "        data_dict['train_path'] = path + \"/train\"\n",
    "        data_dict['val_path'] = path + \"/val\"\n",
    "\n",
    "    train_dataset = MP3D_Habitat_Offline_Dataset(data_dict['train_path'],\n",
    "                                        in_size=data_dict['mp3d_image_input_size'],\n",
    "                                        transform=transform,\n",
    "                                        sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                                        inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                                        cacheItems=data_dict[\"cacheItems\"])\n",
    "    \n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"train_path\"], len(train_dataset), data_dict))\n",
    "    \n",
    "    val_dataset = MP3D_Habitat_Offline_Dataset(data_dict['val_path'],\n",
    "                                        in_size=data_dict['mp3d_image_input_size'],\n",
    "                                        transform=transform,\n",
    "                                        sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                                        inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                                        cacheItems=data_dict[\"cacheItems\"])\n",
    "    \n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"val_path\"], len(val_dataset), data_dict))\n",
    "\n",
    "elif dataset_mode == PtsManipulator.matterport_mode and use_dynamics:\n",
    "    \n",
    "    # THIS IS THE HARDCODED IMAGE SIZE THAT WE SET IN THE HABITAT FRAMEWORK WHEN RENDERING MP3D IMAGES\n",
    "    # THIS DOES NOT CHANGE WHEN WE USE DIFFERENT IMAGE SIZES IN A TRANSFORM OBJECT\n",
    "    # WHEN CHANGING THE IMAGE SIZE IN TRANSFORM OBJECT, THIS GETS REFLECTED IN THE image_size ATTRIBUTE\n",
    "    data_dict['mp3d_image_input_size'] = 256\n",
    "    data_dict['output_from_other_view'] = False\n",
    "    \n",
    "    if not is_on_zerus:\n",
    "        data_dict['train_path'] = path + \"/train\"\n",
    "        data_dict['val_path'] = path + \"/val\"\n",
    "        \n",
    "    # load train datasets\n",
    "    train_datasets = []\n",
    "    for seq in os.listdir(data_dict['train_path']):\n",
    "        path = os.path.join(data_dict['train_path'], seq)\n",
    "        if os.path.isdir(path):\n",
    "            train_datasets.append(MP3D_Habitat_Offline_Dynamic_Dataset(path,\n",
    "                                        in_size=data_dict['mp3d_image_input_size'],\n",
    "                                        output_from_other_view=data_dict['output_from_other_view'],                               \n",
    "                                        transform=transform,\n",
    "                                        sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                                        inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                                        cacheItems=data_dict[\"cacheItems\"]))\n",
    "    \n",
    "    # create merged train dataset\n",
    "    train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "    \n",
    "    print(\"\\nTRAIN DATASET: Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"train_path\"], len(train_dataset), data_dict))\n",
    "    \n",
    "    # load val datasets\n",
    "    val_datasets = []\n",
    "    for seq in os.listdir(data_dict['val_path']):\n",
    "        path = os.path.join(data_dict['val_path'], seq)\n",
    "        if os.path.isdir(path):\n",
    "            val_datasets.append(MP3D_Habitat_Offline_Dynamic_Dataset(path,\n",
    "                                        in_size=data_dict['mp3d_image_input_size'],\n",
    "                                        output_from_other_view=data_dict['output_from_other_view'],  \n",
    "                                        transform=transform,\n",
    "                                        sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                                        inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                                        cacheItems=data_dict[\"cacheItems\"]))\n",
    "            \n",
    "    # create merged val dataset\n",
    "    val_dataset = torch.utils.data.ConcatDataset(val_datasets)\n",
    "    \n",
    "    print(\"\\nVAL DATASET: Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"val_path\"], len(val_dataset), data_dict))\n",
    "    \n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode and not use_dynamics:\n",
    "\n",
    "    data_dict['icl_nuim_output_size'] = image_size\n",
    "    \n",
    "    dataset = ICLNUIMDataset(data_dict['path'],\n",
    "                             transform=transform,\n",
    "                             sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                             inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                             cacheItems=data_dict[\"cacheItems\"], \n",
    "                             out_shape=(image_size, image_size))\n",
    "\n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\".format(data_dict[\"path\"], len(dataset), data_dict))\n",
    "    \n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode and use_dynamics:\n",
    "    \n",
    "    data_dict['icl_nuim_output_size'] = image_size\n",
    "    data_dict['icl_dynamic_output_from_other_view'] = True\n",
    "    \n",
    "    # load train datasets\n",
    "    train_datasets = []\n",
    "    for seq in os.listdir(data_dict['train_path']):\n",
    "        path = os.path.join(data_dict['train_path'], seq)\n",
    "        if os.path.isdir(path):\n",
    "            train_datasets.append(ICLNUIM_Dynamic_Dataset(path,\n",
    "                                 sampleOutput=True,\n",
    "                                 output_from_other_view=data_dict['icl_dynamic_output_from_other_view'], \n",
    "                                 inverse_depth=False,\n",
    "                                 cacheItems=False,\n",
    "                                 transform=transform,\n",
    "                                 out_shape=(image_size, image_size)))\n",
    "    \n",
    "    # create merged train dataset\n",
    "    train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "    \n",
    "    print(\"\\nTRAIN DATASET: Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"train_path\"], len(train_dataset), data_dict))\n",
    "    \n",
    "    # load val datasets\n",
    "    val_datasets = []\n",
    "    for seq in os.listdir(data_dict['val_path']):\n",
    "        path = os.path.join(data_dict['val_path'], seq)\n",
    "        if os.path.isdir(path):\n",
    "            val_datasets.append(ICLNUIM_Dynamic_Dataset(path,\n",
    "                                 sampleOutput=True,\n",
    "                                 output_from_other_view=data_dict['icl_dynamic_output_from_other_view'], \n",
    "                                 inverse_depth=False,\n",
    "                                 cacheItems=False,\n",
    "                                 transform=transform,\n",
    "                                 out_shape=(image_size, image_size)))\n",
    "            \n",
    "    # create merged val dataset\n",
    "    val_dataset = torch.utils.data.ConcatDataset(val_datasets)\n",
    "    \n",
    "    print(\"\\nVAL DATASET: Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"val_path\"], len(val_dataset), data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "aJLyrjl2AyBK",
    "outputId": "9606c677-f3f8-4c01-a7a0-ccede48bb09a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_args = {\n",
    "    \"batch_size\": 4,\n",
    "    \"num_workers\": 4, # Dataset Caching will work only if num_workers = 0. Decide what you like more!\n",
    "    \"random_seed\": 42, # seed random generation for shuffeling indices to always get same images in train/val\n",
    "    \"shuffle_dataset\": True,\n",
    "    **data_dict\n",
    "}\n",
    "\n",
    "np.random.seed(dataset_args[\"random_seed\"])\n",
    "torch.manual_seed(dataset_args[\"random_seed\"])\n",
    "torch.cuda.manual_seed(dataset_args[\"random_seed\"])\n",
    "\n",
    "if dataset_mode == PtsManipulator.matterport_mode:\n",
    "    # For mp3d we have separate train/val folders so we can just create different loaders out of the different datasets\n",
    "\n",
    "    train_len = len(train_dataset)\n",
    "    train_sampler = SubsetRandomSampler(list(range(train_len)))\n",
    "\n",
    "    val_len = len(val_dataset)\n",
    "    val_sampler = SubsetRandomSampler(list(range(val_len)))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=dataset_args[\"batch_size\"], \n",
    "                                               #shuffle=dataset_args[\"shuffle_dataset\"],\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=dataset_args[\"num_workers\"])\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=dataset_args[\"batch_size\"], \n",
    "                                            #shuffle=dataset_args[\"shuffle_dataset\"],\n",
    "                                            sampler=val_sampler,\n",
    "                                            num_workers=dataset_args[\"num_workers\"])\n",
    "\n",
    "\n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode and not use_dynamics:\n",
    "    # Create Train and Val dataset with 80% train and 20% val.\n",
    "    # from: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
    "\n",
    "    # For ICL dataset we do not have train/val datasets so we split the existing dataset 80% to 20%\n",
    "    dataset_args[\"validation_percentage\"] = 0.2\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(dataset_args[\"validation_percentage\"] * dataset_size))\n",
    "    if dataset_args[\"shuffle_dataset\"]:\n",
    "        np.random.seed(dataset_args[\"random_seed\"])\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # #####################\n",
    "    # ICL OVERFITTING CASE:\n",
    "    # #####################\n",
    "    \"\"\"\n",
    "    train_indices = train_indices[:4] # train_indices[0:4] # [train_indices[0]]\n",
    "    val_indices = val_indices[:2]\n",
    "\n",
    "    overfit_item = dataset.__getitem__(train_indices[0])\n",
    "    print(\"OVERFITTING Input Image: {}, Output Image: {}\".format(\n",
    "        train_indices[0],\n",
    "        overfit_item[\"output\"][\"idx\"]))\n",
    "\n",
    "    input_img = overfit_item[\"image\"].cpu().detach().numpy()\n",
    "    output_img = overfit_item[\"output\"][\"image\"].cpu().detach().numpy()\n",
    "    output_seg = overfit_item[\"output\"][\"seg\"].cpu().detach().numpy()\n",
    "\n",
    "    print(torch.min(overfit_item[\"output\"][\"image\"]))\n",
    "    print(torch.max(overfit_item[\"output\"][\"image\"]))\n",
    "    print(overfit_item[\"cam\"])\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(\"OVERFIT TRAIN INPUT IMAGE\")\n",
    "    plt.imshow(np.moveaxis(input_img, 0, -1))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"OVERFIT TRAIN OUTPUT IMAGE\")\n",
    "    plt.imshow(np.moveaxis(output_img, 0, -1))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"OVERFIT TRAIN OUTPUT SEG\")\n",
    "    plt.imshow(np.moveaxis(output_seg, 0, -1))\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    # #########################\n",
    "    # END ICL OVERFITTING CASE\n",
    "    # #########################\n",
    "    \n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=dataset_args[\"batch_size\"], \n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=dataset_args[\"num_workers\"])\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                    batch_size=dataset_args[\"batch_size\"],\n",
    "                                                    sampler=valid_sampler,\n",
    "                                                    num_workers=dataset_args[\"num_workers\"])\n",
    "\n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode and use_dynamics:\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=dataset_args[\"batch_size\"], \n",
    "                                               shuffle=dataset_args[\"shuffle_dataset\"],\n",
    "                                               num_workers=dataset_args[\"num_workers\"])\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                               batch_size=dataset_args[\"batch_size\"], \n",
    "                                               shuffle=dataset_args[\"shuffle_dataset\"],\n",
    "                                               num_workers=dataset_args[\"num_workers\"])\n",
    "\n",
    "    '''SHOW TEST ITEM'''\n",
    "    '''\n",
    "    item = train_dataset.__getitem__(600)\n",
    "\n",
    "    input_img = item[\"image\"].cpu().detach().numpy()\n",
    "    output_img = item[\"output\"][\"image\"].cpu().detach().numpy()\n",
    "    output_gt_moved_img = item[\"output\"]['gt_moved_rgb_for_evaluation_only'].cpu().detach().numpy()\n",
    "    output_seg = item[\"output\"][\"seg\"].cpu().detach().numpy()\n",
    "\n",
    "    print(torch.min(item[\"output\"][\"image\"]))\n",
    "    print(torch.max(item[\"output\"][\"image\"]))\n",
    "    print(item[\"cam\"])\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(\"TRAIN INPUT IMAGE\")\n",
    "    plt.imshow(np.moveaxis(input_img, 0, -1))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"TRAIN OUTPUT IMAGE\")\n",
    "    plt.imshow(np.moveaxis(output_img, 0, -1))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"TRAIN OUTPUT MOVED IMAGE\")\n",
    "    plt.imshow(np.moveaxis(output_gt_moved_img, 0, -1))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"TRAIN OUTPUT SEG\")\n",
    "    plt.imshow(np.moveaxis(output_seg, 0, -1))\n",
    "    plt.show()\n",
    "    \n",
    "    img = np.moveaxis(item['image'].numpy(), 0, -1)\n",
    "    print(\"Mask dynamics at input\")\n",
    "    img[:,:] = np.array([0, 0, 0])\n",
    "    mask = np.moveaxis(item[\"dynamics\"][\"input_mask\"].numpy(), 0, -1).squeeze()\n",
    "    img[mask == 1] = np.array([1, 1, 1])\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Mask dynamics at output\")\n",
    "    img[:, :] = np.array([0, 0, 0])\n",
    "    mask = np.moveaxis(item[\"dynamics\"][\"output_mask\"].numpy(), 0, -1).squeeze()\n",
    "    img[mask == 1] = np.array([1, 1, 1])\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "    \n",
    "dataset_args[\"train_len\"] = len(train_loader)\n",
    "dataset_args[\"val_len\"] = len(validation_loader)\n",
    "\n",
    "print(\"Dataset parameters: {}\".format(dataset_args))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKgCXiSPZgY1"
   },
   "source": [
    "# Model & Loss Init\n",
    "\n",
    "Instantiate and initialize NovelViewSynthesisModel and a selected flavor of SynthesisLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "hzBJYgAlZgY2",
    "outputId": "2a84c6d9-5449-4e0a-c902-c6d4722789ad"
   },
   "outputs": [],
   "source": [
    "# TODO: Define more parameters in the dict according to availalbe ones in the model, as soon as they are needed.\n",
    "# Right now we just use the default parameters for the rest (see outcommented list or the .py file)\n",
    "    \n",
    "model_args={\n",
    "    'imageSize': image_size, # change this now in the first dataloading cell from above!\n",
    "    \n",
    "    'use_gt_depth': True,\n",
    "    'normalize_images': False,\n",
    "    'use_rgb_features': False,\n",
    "\n",
    "    'num_depth_filters': 32, #1, 16, 32\n",
    "    \n",
    "    'enc_dims': [3, 16, 16, 16, 32, 32, 32, 32, 64],\n",
    "    'enc_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 64],\n",
    "    #'enc_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'enc_dims': [3, 8, 8],\n",
    "    #'enc_blk_types': [\"id\", \"id\"],\n",
    "    'enc_noisy_bn': True,\n",
    "    'enc_spectral_norm': True,\n",
    "    \n",
    "    'dec_activation_func': nn.Sigmoid(),\n",
    "    'dec_dims': [64, 32, 128, 128, 64, 64, 64, 64, 3],\n",
    "    'dec_blk_types': [\"id\", \"avg\", \"avg\", \"id\", \"ups\", \"ups\", \"id\", \"id\"],\n",
    "    #'dec_dims': [64, 32, 32, 16, 16, 8, 8, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'dec_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 32, 32, 16, 16, 8, 8, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'dec_dims': [3, 16, 32, 64, 64, 64, 64, 64, 32, 128, 128, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"avg\", \"avg\", \"id\", \"ups\", \"ups\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    'seg_dims': [3, 32, 64, 128, 128, 64, 64, 64, 64, 32, 3],\n",
    "    'seg_blk_types': ['id', 'id', 'avg', 'avg', 'id', 'ups', 'ups', 'id', 'id', 'id'],\n",
    "    'refinement_mode': 'sequential', # sequential, parallel. If sequential \"shared_layers\" will have no effect.\n",
    "    'concat_input_seg': True, # currently, can only be used when refinement_mode=sequential\n",
    "    'shared_layers': 0, # only relevant when refinement_mode=parallel\n",
    "    'dec_noisy_bn': [True, False],  #first is for decoder, second for segmentation network\n",
    "    'dec_spectral_norm': [True, False],\n",
    "                      \n",
    "    'projection_mode': dataset_mode,\n",
    "    \n",
    "    # from here attributes for the SynthesisLossRGBandSeg of the nvs_model\n",
    "    'rgb_l1_loss': '1.0_l1',\n",
    "    'rgb_content_loss': '10.0_content', # synsin default: 10.0\n",
    "    'seg_l1_loss': '1.0_l1', # this is only relevant for training without dynamics, will be ignored otherwise\n",
    "    'seg_content_loss': '0.0_content', # this is only relevant for training without dynamics, will be ignored otherwise\n",
    "    \n",
    "    # from here attributes for the SceneEditingLoss of the nvs_model\n",
    "    'lp_region_lambda': 3.0,\n",
    "    'lp_region_params': [1.0, 1.0, 1.0], # parameters: [p, weight_outside_movement, weight_inside_movement]\n",
    "    'lp_region_ignore_input_mask_region': True # if true: the area of movement from which object was moved away will not count in the loss (the output region takes precedence --> if they overlap the whole output region still counts and only input region without the overlap will not be considered)\n",
    "}\n",
    "\n",
    "# keep these loss object constant and modify usage of losses by e.g. setting one coefficient to 0\n",
    "if not use_dynamics:\n",
    "    \n",
    "    nvs_loss = SynthesisLossRGBandSeg(rgb_losses=[model_args['rgb_l1_loss'], model_args['rgb_content_loss']],\n",
    "                                      seg_losses=[model_args['seg_l1_loss'], model_args['seg_content_loss']])\n",
    "    \n",
    "else:\n",
    "\n",
    "    nvs_loss = SceneEditingAndSynthesisLoss(\n",
    "        synthesis_losses=[model_args['rgb_l1_loss'], model_args['rgb_content_loss']],\n",
    "        scene_editing_weight=model_args['lp_region_lambda'],\n",
    "        scene_editing_lpregion_params=model_args['lp_region_params'],\n",
    "        scene_editing_ignore_input_mask_region=model_args['lp_region_ignore_input_mask_region']\n",
    "    )\n",
    "\n",
    "model = NovelViewSynthesisModel(imageSize=model_args['imageSize'],\n",
    "                                \n",
    "                                max_z=10.0,\n",
    "                                min_z=0.1,\n",
    "                                num_filters=model_args['num_depth_filters'],\n",
    "                                \n",
    "                                enc_dims=model_args['enc_dims'],\n",
    "                                enc_blk_types=model_args['enc_blk_types'],\n",
    "                                enc_noisy_bn=model_args['enc_noisy_bn'],\n",
    "                                enc_spectral_norm=model_args['enc_spectral_norm'],\n",
    "                                \n",
    "                                dec_dims=model_args['dec_dims'],\n",
    "                                dec_blk_types=model_args['dec_blk_types'],\n",
    "                                seg_dims=model_args['seg_dims'],\n",
    "                                seg_blk_types=model_args['seg_blk_types'],\n",
    "                                refinement_mode=model_args['refinement_mode'],\n",
    "                                concat_input_seg=model_args['concat_input_seg'],\n",
    "                                shared_layers=model_args['shared_layers'],\n",
    "                                dec_activation_func=model_args['dec_activation_func'],\n",
    "                                dec_noisy_bn=model_args['dec_noisy_bn'],\n",
    "                                dec_spectral_norm=model_args['dec_spectral_norm'],\n",
    "                                \n",
    "                                projection_mode=model_args['projection_mode'],\n",
    "                                points_per_pixel=16,\n",
    "                                #learn_feature=True,\n",
    "                                radius=4.0,\n",
    "                                #rad_pow=2,\n",
    "                                #accumulation='alphacomposite',\n",
    "                                #accumulation_tau=1,\n",
    "                                \n",
    "                                use_rgb_features=model_args['use_rgb_features'],\n",
    "                                use_gt_depth=model_args['use_gt_depth'],\n",
    "                                #use_inverse_depth=False,\n",
    "                                normalize_images=model_args['normalize_images'])\n",
    "model_args[\"model\"] = type(model).__name__\n",
    "\n",
    "print(\"Model configuration: {}\".format(model_args))\n",
    "\n",
    "#print(\"Architecture:\", model)\n",
    "print(\"Total number of paramaters:\", count_parameters(model))\n",
    "print(\"Parameters ENCODER:\", count_parameters(model.encoder))\n",
    "print(\"Parameters DEPTH:\", count_parameters(model.pts_regressor))\n",
    "print(\"Parameters DECODER:\", count_parameters(model.projector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeFz-j00u5LR"
   },
   "source": [
    "# Training Visualization\n",
    "\n",
    "Start Tensorboard for visualization of the upcoming training / validation / test steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard. Might need to make sure, that the correct runs directory is chosen here.\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir \"../runs\"\n",
    "#!tensorboard --logdir ../runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zet9gPxvM2i"
   },
   "source": [
    "# Training\n",
    "\n",
    "Start training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvDPM-hKJGlt"
   },
   "outputs": [],
   "source": [
    "# This flag decides with solver gets used and where the logs will be logged into (into which directory)\n",
    "train_with_discriminator = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OTnq2RYJy4Dn",
    "outputId": "801ce2ea-143e-4cf9-daac-59e41081484d"
   },
   "outputs": [],
   "source": [
    "# Create unique ID for this training process for saving to disk.\n",
    "\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "now = datetime.now() # current date and time\n",
    "id = str(uuid.uuid1())\n",
    "id_suffix = now.strftime(\"%Y-%b-%d_%H-%M-%S\") + \"_\" + id\n",
    "\n",
    "if train_with_discriminator:\n",
    "    log_dir_name = \"Full_GAN\"\n",
    "else:\n",
    "    log_dir_name = \"Full_No_GAN\"\n",
    "\n",
    "log_dir = \"../runs/\" + log_dir_name + \"/\" + id_suffix # Might need to make sure, that the correct runs directory is chosen here.\n",
    "print(\"log_dir:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "QY38vRjuAyBc",
    "outputId": "ea99eac0-20b3-4b69-e5c4-03d91fbae721"
   },
   "outputs": [],
   "source": [
    "# Configure solver\n",
    "extra_args = {\n",
    "    **model_args,\n",
    "    **dataset_args,\n",
    "    'num_D': 2, # number of discriminators, each downsamples by 2\n",
    "    'size_D': 64, # number of channels each conv in the discriminator has\n",
    "    'loss_D': 'original', # discriminator loss, options are original(cross-entropy), ls (MSE), hinge, w\n",
    "    'no_feature_loss': False, # if discriminator should not use feature loss\n",
    "    'init_weights': True,\n",
    "    'lr_step': 15, #number of epochs after which the learning rate is mulitplied with gamma\n",
    "    'lr_gamma': 0.1\n",
    "}\n",
    "\n",
    "if train_with_discriminator:\n",
    "    solver = GAN_Wrapper_Solver(optim_d=torch.optim.Adam,\n",
    "                                optim_d_args={\"lr\": 1e-4,\n",
    "                                              \"betas\": (0.9, 0.999),\n",
    "                                              \"eps\": 1e-8,\n",
    "                                              \"weight_decay\": 0.0},# is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                                optim_g=torch.optim.Adam,\n",
    "                                optim_g_args={\"lr\": 1e-4,\n",
    "                                              \"betas\": (0.9, 0.999),\n",
    "                                              \"eps\": 1e-8,\n",
    "                                              \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                                g_loss_func=nvs_loss,\n",
    "                                extra_args=extra_args,\n",
    "                                log_dir=log_dir,\n",
    "                                num_D=extra_args['num_D'],\n",
    "                                size_D=extra_args['size_D'],\n",
    "                                loss_D=extra_args['loss_D'],\n",
    "                                no_gan_feature_loss=extra_args['no_feature_loss'],\n",
    "                                init_discriminator_weights=extra_args['init_weights'],\n",
    "                                lr_step=extra_args['lr_step'],\n",
    "                                lr_gamma=extra_args['lr_gamma'])\n",
    "else:\n",
    "    solver = NVS_Solver(optim=torch.optim.Adam,\n",
    "                        optim_args={\"lr\": 1e-5,\n",
    "                                    \"betas\": (0.9, 0.999),\n",
    "                                    \"eps\": 1e-8,\n",
    "                                    \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html,\n",
    "                        loss_func=nvs_loss,\n",
    "                        extra_args=extra_args,\n",
    "                        tensorboard_writer=None, # let solver create a new instance\n",
    "                        log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqpqX4mUBJyw"
   },
   "source": [
    "### Load Pretrained Segmentation Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50HDqZd4BEjw"
   },
   "outputs": [],
   "source": [
    "def save_model(modelname, model):\n",
    "    from pathlib import Path\n",
    "    Path(\"../saved_models\").mkdir(parents=True, exist_ok=True)\n",
    "    # Might need to make sure, that the correct saved_results directory is chosen here.\n",
    "    filepath = \"../saved_models/\" + modelname + \".pt\"\n",
    "    torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YFKVhDZtBIwS",
    "outputId": "6e3b63b4-ef57-4382-9893-9d4021548522"
   },
   "outputs": [],
   "source": [
    "model_path = \"/content/drive/My Drive/Novel_View_Synthesis/saved_models/SegNet_2020-Aug-03_11-31-44_e8970370-d57c-11ea-896e-0242ac1c0002.pt\"\n",
    "\n",
    "model.projector.seg_blocks.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gu6myJkTBtNr"
   },
   "source": [
    "#### Freeze Weights of Segmentation part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHoUQqt2B-t0"
   },
   "outputs": [],
   "source": [
    "for params in model.projector.seg_blocks:\n",
    "    params.requires_grad_ = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "266725042d35486d9fee9798e2f4946f",
      "a406677e06664a9d9c108001f0292b8f",
      "fa8ccf884ac44db696224e343b488e41",
      "6d62ee590d004159980e67443b4897bf",
      "c809b50a64794724aa20c1024ffd6894",
      "130b13445b334d869b6656b8221f204b",
      "9c7036dfac2d4e1e965e7707c8cc9fa9",
      "21ad505ee1ee4d1aa65a5d6d48c87460"
     ]
    },
    "colab_type": "code",
    "id": "1IhNe6ynzXox",
    "outputId": "a030153b-9ad7-4b66-824b-88b713a5c6bf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "\n",
    "num_epochs=20\n",
    "log_nth_iter=0\n",
    "log_nth_epoch=1\n",
    "tqdm_mode='epoch'\n",
    "'''\n",
    "tqdm_mode:\n",
    "    'total': tqdm log how long all epochs will take,\n",
    "    'epoch': tqdm for each epoch how long it will take,\n",
    "    anything else, e.g. None: do not use tqdm\n",
    "'''\n",
    "\n",
    "'''\n",
    "Use CUDA_VISIBLE_DEVICES=0,1,2 jupyter notebook etc depending on how many gpu you want to use.\n",
    "The model will then be replicated on each gpu and the batches are split between them. \n",
    "So if you use 2 gpus you could make the batch_size twice as large as before.\n",
    "good for processing a lot of data quickly\n",
    "'''\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using multiple GPUs!!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# TODO: Add parameters to extra_args dict?\n",
    "if train_with_discriminator:\n",
    "    steps = 1 # how many steps of training for discriminator/generator before switching to generator/discriminator\n",
    "    solver.train(model,\n",
    "                 train_loader, \n",
    "                 validation_loader,\n",
    "                 num_epochs=num_epochs,\n",
    "                 log_nth_iter=log_nth_iter,\n",
    "                 log_nth_epoch=log_nth_epoch,\n",
    "                 tqdm_mode=tqdm_mode,\n",
    "                 steps=steps)\n",
    "else:\n",
    "    solver.train(model,\n",
    "                 train_loader,\n",
    "                 validation_loader,\n",
    "                 num_epochs=num_epochs,\n",
    "                 log_nth_iter=log_nth_iter,\n",
    "                 log_nth_epoch=log_nth_epoch,\n",
    "                 tqdm_mode=tqdm_mode,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVWNikT4PvGj"
   },
   "outputs": [],
   "source": [
    "# To download tensorboard runs from Colab\n",
    "\n",
    "# TODO: Make sure that only new ones are copied --> for tensorboard runs on colab, do not use git repository as \"runs\" directory?\n",
    "# TODO: Instead of downloading, directly move it to the git repository that is currently checked out and push changes?\n",
    "if is_on_colab:\n",
    "  from google.colab import files\n",
    "  !zip -r /content/runs.zip /content/runs\n",
    "  files.download(\"/content/runs.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2VDewrSvags"
   },
   "source": [
    "# Test\n",
    "\n",
    "Test with test dataset.\n",
    "Will load the data and start the training.\n",
    "\n",
    "Visualizations can be seen in Tensorboard above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8S9-1x0zbHZ"
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "# TODO: Find real test split, for now we load the SAME dataset as for train/val (just that this notebook is complete...)\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "if is_on_zerus:\n",
    "    test_path = \"/mnt/raid/teampc/ICL-NUIM/office_room_traj2_loop\"\n",
    "\n",
    "if is_on_colab:\n",
    "    test_path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/living_room_traj2_loop\"\n",
    "\n",
    "test_dataset = ICLNUIMDataset(test_path, transform=transform) # TODO also use rest of parameters...\n",
    "\n",
    "test_indices = list(range(len(test_dataset)))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "test_sampler = SubsetRandomSampler(test_indices[:len(test_indices)//10])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=dataset_args[\"batch_size\"], \n",
    "                                          sampler=test_sampler,\n",
    "                                          num_workers=4)\n",
    "\n",
    "print(\"Length of test set: {}\".format(len(test_loader)))\n",
    "print(\"Loaded test set: {}\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cl2sFO4Kynp6"
   },
   "outputs": [],
   "source": [
    "# Start testing\n",
    "\n",
    "solver.test(model, test_loader, test_prefix=\"icl_test\", log_nth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GVBR6gUi_lJb"
   },
   "source": [
    "## Evaluate on an ICL dynamics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W9y6z_dy_lJc"
   },
   "outputs": [],
   "source": [
    "sequence = \"seq0001\" # indices [1-3] are available\n",
    "\n",
    "if is_on_colab:\n",
    "    test_path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/custom/\" + sequence\n",
    "        \n",
    "elif is_on_zerus:\n",
    "    test_path = \"/mnt/raid/teampc/ICL-NUIM/custom/\" + sequence\n",
    "        \n",
    "else:\n",
    "    test_path = \"/home/lukas/Desktop/datasets/ICL-NUIM/custom/\" + sequence\n",
    "    \n",
    "test_dataset = ICLNUIM_Dynamic_Dataset(test_path,\n",
    "                             sampleOutput=True,\n",
    "                             output_from_other_view=data_dict['icl_dynamic_output_from_other_view'], \n",
    "                             inverse_depth=False,\n",
    "                             cacheItems=False,\n",
    "                             transform=transform,\n",
    "                             out_shape=(image_size, image_size))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, \n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4)\n",
    "\n",
    "print(\"Length of test set: {}\".format(len(test_loader)))\n",
    "print(\"Loaded test set: {}\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_j0xtm3V_lJq"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader)):\n",
    "        _, out, _ = solver.nvs_solver.forward_pass(model, batch)\n",
    "        pred_img = out[\"PredImg\"].squeeze().permute((1,2,0)).cpu().detach().numpy()\n",
    "        pred_depth = out[\"PredDepth\"].squeeze().cpu().detach().numpy()\n",
    "        plt.imshow(pred_img)\n",
    "        plt.show()\n",
    "        plt.imshow(pred_depth)\n",
    "        plt.show()\n",
    "        \n",
    "        imageio.imwrite(log_dir + '/pred_img_'+str(i)+'.png', pred_img)\n",
    "        imageio.imwrite(log_dir + '/pred_depth_'+str(i)+'.png', pred_depth)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlzKn0dWTNoU"
   },
   "source": [
    "## Generating a Test Time Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKHYULewTNoV"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "from util.nvs_solver import to_cuda, default_batch_loader\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the model if needed by using the last cell\n",
    "\n",
    "model.eval().cuda()\n",
    "\n",
    "# Pick an image for the first frame and extract items related to it\n",
    "test_item_idx = 0\n",
    "test_item = test_dataset.__getitem__(test_item_idx)\n",
    "input_img, K, K_inv, input_RT, input_RT_inv, output_RT, output_RT_inv, gt_img, depth_img = to_cuda(default_batch_loader(test_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DkK_yJpTNoX"
   },
   "source": [
    "### Rotation & Translation\n",
    "Use the sliders to jointly modify every axis and the rotation around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxmWFW1rTNoX"
   },
   "outputs": [],
   "source": [
    "# Keep modified RT2 matrices\n",
    "traj = []\n",
    "\n",
    "# A function to generate translational trajectory, modifies output_RT, output_RT_inv & gt_img\n",
    "def modify_frame(x=0,y=0,z=0, rx=0, ry=0, rz=0):\n",
    "    global traj, input_img, K, K_inv, input_RT, input_RT_inv, output_RT, output_RT_inv, gt_img, depth_img\n",
    "    R_X = torch.Tensor([\n",
    "      [ 1.0,  0.0, 0.0, x*255],\n",
    "      [ 0.0,  math.cos(rx*3.1415/180), -math.sin(rx*3.1415/180)/255, y*255],\n",
    "      [ 0.0,  math.sin(rx*3.1415/180)/255, math.cos(rx*3.1415/180), z],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    R_Y = torch.Tensor([\n",
    "      [ math.cos(ry*3.1415/180),  0.0, math.sin(ry*3.1415/180)/255, 0.0],\n",
    "      [ 0.0,  1.0, 0.0, 0.0],\n",
    "      [ -math.sin(ry*3.1415/180)/255,  0.0, math.cos(ry*3.1415/180), 0.0],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    R_Z = torch.Tensor([\n",
    "      [ math.cos(rz*3.1415/180),  -math.sin(rz*3.1415/180), 0.0, 0.0],\n",
    "      [ math.sin(rz*3.1415/180),  math.cos(rz*3.1415/180), 0.0, 0.0],\n",
    "      [ 0.0,  0.0, 1.0, 0.0],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    # Translate input_RT by given x,y,z\n",
    "    output_RT_inv = (R_X@R_Y@R_Z).mm(input_RT_inv)\n",
    "\n",
    "    # Perform projection to obtain a pseudo GT for the manipulation\n",
    "    gt_img = model.pts_transformer.forward_justpts(\n",
    "        input_img.unsqueeze(0),\n",
    "        depth_img.unsqueeze(0),\n",
    "        K.unsqueeze(0),\n",
    "        K_inv.unsqueeze(0),\n",
    "        input_RT.unsqueeze(0),\n",
    "        input_RT_inv.unsqueeze(0),\n",
    "        output_RT.unsqueeze(0),\n",
    "        output_RT_inv.unsqueeze(0),\n",
    "    )\n",
    "    print(\"Projection with new RT:\")\n",
    "    gt_img_np = gt_img.squeeze(0).cpu().detach().numpy()\n",
    "    plt.imshow(np.moveaxis(gt_img_np, 0, -1))\n",
    "    plt.show()\n",
    "    \n",
    "    # Store matrices for the new view\n",
    "    traj.append((output_RT, output_RT_inv, gt_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfRU_kyhTNoZ"
   },
   "outputs": [],
   "source": [
    "matrix = interact(modify_frame, \n",
    "                  x=(-1.0,1.0),\n",
    "                  y=(-1.0,1.0), \n",
    "                  z=(-1.0,1.0), \n",
    "                  rx=(-10.0,10.0, 1), \n",
    "                  ry=(-10.0,10.0, 1),\n",
    "                  rz=(-10.0,10.0, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyUp6f9bTNoc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(test_item[\"cam\"][\"RT1\"])\n",
    "# traj.pop(0) # Interactive slider sometimes first item has the same RT matrix as the input view (RT1), discard it\n",
    "pprint(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2Vsd4A3TNoe"
   },
   "outputs": [],
   "source": [
    "# It is important to pass this image to loader to apply same transforms that was applied during training.\n",
    "# We have to make sure that test time images get the same transforms as train time to have meaningful results.\n",
    "test_sampler = SubsetRandomSampler([test_item_idx])\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                          sampler=test_sampler, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the frame (triggers get_item and transforms)\n",
    "    test_item = next(iter(test_loader))\n",
    "    test_item = to_cuda(default_batch_loader(test_item)) # List of contents in dict\n",
    "    # For each different RT matrix perform a forward pass using GT depth\n",
    "    for output_RT, output_RT_inv, gt_img in traj:\n",
    "        out = model(test_item[0], # input_img\n",
    "                    test_item[1], # K\n",
    "                    test_item[2], # K_inv\n",
    "                    test_item[3], # input_RT\n",
    "                    test_item[4], # input_RT_inv\n",
    "                    output_RT.unsqueeze(0), \n",
    "                    output_RT_inv.unsqueeze(0), \n",
    "                    gt_img,                     # Not used\n",
    "                    test_item[-1]\n",
    "                   )              # GT depth\n",
    "        # Visualize prediction by network\n",
    "        pred = out[\"PredImg\"]\n",
    "        pred_np = pred.squeeze().cpu().detach().numpy()\n",
    "        plt.imshow(np.moveaxis(pred_np, 0, -1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnwhwcXu_lJ6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NN-Search usage: generate trajectories and let NN run. \n",
    "generate gif from images.\n",
    "'''\n",
    "min_idx = []\n",
    "for rts in traj:\n",
    "    norm1 = np.linalg.norm(rts[1].cpu().numpy())\n",
    "    min_diff = 100\n",
    "    for i,elem in enumerate(test_loader):\n",
    "        # adjust these values to limit the range of the nearest neighbour search \n",
    "        if i >= 300 and i < 400:\n",
    "            norm2 = np.linalg.norm(elem['cam']['RT2inv'])\n",
    "            diff = np.absolute(np.absolute(norm1) - np.absolute(norm2))\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                min_id = i\n",
    "    min_idx.append(min_id)\n",
    "    \n",
    "for ids in min_idx:\n",
    "    item = test_dataset.__getitem__(ids)\n",
    "    img = item['image']\n",
    "    plt.imshow(np.moveaxis(img, 0, -1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTLUJGpnBpud"
   },
   "source": [
    "# Save the model\n",
    "\n",
    "Save network with its weights to disk.\n",
    "\n",
    "See torch.save function: https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models \n",
    "\n",
    "Load again with `the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_zjMVB7Bpue"
   },
   "outputs": [],
   "source": [
    "def save_model(modelname, model):\n",
    "    from pathlib import Path\n",
    "    Path(\"../saved_models\").mkdir(parents=True, exist_ok=True)\n",
    "    # Might need to make sure, that the correct saved_results directory is chosen here.\n",
    "    filepath = \"../saved_models/\" + modelname + \".pt\"\n",
    "    torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JfoL3IHBpuv"
   },
   "outputs": [],
   "source": [
    "nvs_modelname = \"nvs_\" + id_suffix\n",
    "save_model(nvs_modelname, model)\n",
    "\n",
    "if train_with_discriminator:\n",
    "    # Also save the discriminator - currently this can only be accessed through the solver (change it!)\n",
    "    gan_modelname = \"gan_\" + id_suffix\n",
    "    save_model(gan_modelname, solver.netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwbC3OFOEmLX"
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL AGAIN for verification purposes\n",
    "# Should print: <All keys matched successfully> per each model if it works\n",
    "\n",
    "new_model=True\n",
    "# add a different model name to be loaded here\n",
    "if new_model:\n",
    "    nvs_modelname = \"nvs_2020-Aug-21_19-41-12_815e5660-e3d5-11ea-9c12-25fdc0bd981e\"\n",
    "    gan_modelname = \"gan_2020-Aug-21_19-41-12_815e5660-e3d5-11ea-9c12-25fdc0bd981e\"\n",
    "    \n",
    "nvs_filepath = \"../saved_models/\" + nvs_modelname + \".pt\"\n",
    "print(\"NVS_Model loading: \", model.load_state_dict(torch.load(nvs_filepath)))\n",
    "\n",
    "if train_with_discriminator:\n",
    "    gan_filepath = \"../saved_models/\" + gan_modelname + \".pt\"\n",
    "    print(\"Discriminator loading: \", solver.netD.load_state_dict(torch.load(gan_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy91cvNeJGmc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "entire_network_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "130b13445b334d869b6656b8221f204b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21ad505ee1ee4d1aa65a5d6d48c87460": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "266725042d35486d9fee9798e2f4946f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa8ccf884ac44db696224e343b488e41",
       "IPY_MODEL_6d62ee590d004159980e67443b4897bf"
      ],
      "layout": "IPY_MODEL_a406677e06664a9d9c108001f0292b8f"
     }
    },
    "6d62ee590d004159980e67443b4897bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21ad505ee1ee4d1aa65a5d6d48c87460",
      "placeholder": "",
      "style": "IPY_MODEL_9c7036dfac2d4e1e965e7707c8cc9fa9",
      "value": " 200/200 [07:30&lt;00:00,  2.25s/it]"
     }
    },
    "9c7036dfac2d4e1e965e7707c8cc9fa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a406677e06664a9d9c108001f0292b8f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c809b50a64794724aa20c1024ffd6894": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fa8ccf884ac44db696224e343b488e41": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_130b13445b334d869b6656b8221f204b",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c809b50a64794724aa20c1024ffd6894",
      "value": 200
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
